# -*- coding: utf-8 -*-
"""final misinfo classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1i_ccyup54-42D6t_rs2t6guNmWKaayQv
"""

#-----------------------------------------------------
# 1. Mount Drive
#-----------------------------------------------------
from google.colab import drive
drive.mount('/content/drive')

!pip install easyocr pillow

!pip install transformers torch torchvision datasets

!pip install -q transformers timm datasets sentence-transformers faiss-cpu torch torchvision fastapi uvicorn[standard] pymongo python-multipart aiohttp nest-asyncio pyngrok

import os
ROOT = "/content/drive/MyDrive/liar's dataset"
PROCESSED = os.path.join(ROOT, "processed")
os.makedirs(PROCESSED, exist_ok=True)

print("Root:", ROOT)
print("Processed path:", PROCESSED)

import pandas as pd, os, sys

processed = PROCESSED
text_csv = os.path.join(processed, "text_dataset.csv")
img_train_csv = os.path.join(processed, "image_train.csv")
img_test_csv  = os.path.join(processed, "image_test.csv")

for p in [text_csv, img_train_csv, img_test_csv]:
    if not os.path.exists(p):
        print("Missing:", p)
print("If any file missing, upload / create them in Drive and re-run this cell.")
if os.path.exists(text_csv):
    df = pd.read_csv(text_csv)
    print("Text dataset:", df.shape)
    print(df.head(3))
if os.path.exists(img_train_csv):
    dfimg = pd.read_csv(img_train_csv)
    print("Image train:", dfimg.shape, dfimg.columns.tolist())
    print(dfimg.head(3))
if os.path.exists(img_test_csv):
    dfimg2 = pd.read_csv(img_test_csv)
    print("Image test:", dfimg2.shape, dfimg2.columns.tolist())
    print(dfimg2.head(3))

import pandas as pd, os, sys

processed = PROCESSED
text_csv = os.path.join(processed, "text_dataset.csv")
img_train_csv = os.path.join(processed, "image_train.csv")
img_test_csv  = os.path.join(processed, "image_test.csv")

for p in [text_csv, img_train_csv, img_test_csv]:
    if not os.path.exists(p):
        print("Missing:", p)
print("If any file missing, upload / create them in Drive and re-run this cell.")
if os.path.exists(text_csv):
    df = pd.read_csv(text_csv)
    print("Text dataset:", df.shape)
    print(df.head(3))
if os.path.exists(img_train_csv):
    dfimg = pd.read_csv(img_train_csv)
    print("Image train:", dfimg.shape, dfimg.columns.tolist())
    print(dfimg.head(3))
if os.path.exists(img_test_csv):
    dfimg2 = pd.read_csv(img_test_csv)
    print("Image test:", dfimg2.shape, dfimg2.columns.tolist())
    print(dfimg2.head(3))

import torch, os, random, numpy as np
from sklearn.model_selection import train_test_split
from transformers import BertTokenizer, BertForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F
from tqdm.auto import tqdm

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", DEVICE)

TEXT_MAX_LEN = 64
BATCH_SIZE_TEXT = 8
EPOCHS_TEXT = 3
LR_TEXT = 2e-5
MODEL_SAVE_DIR = os.path.join(PROCESSED, "bert_text")
os.makedirs(MODEL_SAVE_DIR, exist_ok=True)

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len=64):
        self.texts = [str(t) for t in texts]
        self.labels = labels.astype(int).tolist()
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self): return len(self.texts)

    def __getitem__(self, idx):
        t = self.texts[idx]
        enc = self.tokenizer(t, padding='max_length', truncation=True,
                             max_length=self.max_len, return_tensors='pt')
        return {
            'input_ids': enc['input_ids'].squeeze(0),
            'attention_mask': enc['attention_mask'].squeeze(0),
            'label': torch.tensor(self.labels[idx], dtype=torch.long)
        }

df = pd.read_csv(text_csv)
df = df.dropna(subset=['text', 'label'])
train_df, temp_df = train_test_split(df, test_size=0.2, stratify=df['label'], random_state=SEED)
val_df, test_df = train_test_split(temp_df, test_size=0.5, stratify=temp_df['label'], random_state=SEED)
#testing on small size of small as it is easy to run on cpu
print("Train/Val/Test sizes:", len(train_df), len(val_df), len(test_df))
train_ds = TextDataset(train_df['text'], train_df['label'], tokenizer, TEXT_MAX_LEN)
val_ds   = TextDataset(val_df['text'], val_df['label'], tokenizer, TEXT_MAX_LEN)
test_ds  = TextDataset(test_df['text'], test_df['label'], tokenizer, TEXT_MAX_LEN)

train_ds_small = torch.utils.data.Subset(train_ds, range(min(200, len(train_ds))))
val_ds_small   = torch.utils.data.Subset(val_ds, range(min(100, len(val_ds))))
test_ds_small  = torch.utils.data.Subset(test_ds, range(min(100, len(test_ds))))

train_loader = DataLoader(train_ds_small, batch_size=BATCH_SIZE_TEXT, shuffle=True, num_workers=2, pin_memory=True)
val_loader   = DataLoader(val_ds_small, batch_size=BATCH_SIZE_TEXT, shuffle=False, num_workers=2, pin_memory=True)
test_loader  = DataLoader(test_ds_small, batch_size=BATCH_SIZE_TEXT, shuffle=False, num_workers=2, pin_memory=True)

def train_bert():
    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    model.to(DEVICE)

    from torch.nn import CrossEntropyLoss
    real_count = (train_df['label'] == 1).sum()
    fake_count = (train_df['label'] == 0).sum()
    weight = torch.tensor([1.0, fake_count/real_count]).to(DEVICE)
    criterion = CrossEntropyLoss(weight=weight)


    optimizer = AdamW(model.parameters(), lr=LR_TEXT)
    total_steps = len(train_loader) * EPOCHS_TEXT
    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(0.1*total_steps), num_training_steps=total_steps)

    scaler = torch.cuda.amp.GradScaler() if DEVICE.type=='cuda' else None

    best_val_f1 = 0.0
    for epoch in range(EPOCHS_TEXT):
        model.train()
        pbar = tqdm(train_loader, desc=f"Train Epoch {epoch+1}")
        total_loss = 0.0
        for batch in pbar:
            optimizer.zero_grad()
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            labels = batch['label'].to(DEVICE)

            if scaler:
                with torch.cuda.amp.autocast():
                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                    loss = outputs.loss
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
                loss = outputs.loss
                loss.backward()
                optimizer.step()

            scheduler.step()
            total_loss += loss.item()
            pbar.set_postfix(loss=(total_loss / (pbar.n+1)))

        val_metrics = evaluate_bert(model, val_loader)
        print(f"Epoch {epoch+1} Val Acc: {val_metrics['acc']:.4f} F1: {val_metrics['f1']:.4f}")
        if val_metrics['f1'] > best_val_f1:
            best_val_f1 = val_metrics['f1']
            ckpt = os.path.join(MODEL_SAVE_DIR, "bert_best.pt")
            torch.save(model.state_dict(), ckpt)
            print("Saved checkpoint:", ckpt)
    model.load_state_dict(torch.load(os.path.join(MODEL_SAVE_DIR, "bert_best.pt")))
    test_metrics = evaluate_bert(model, test_loader)
    print("TEST:", test_metrics)
    return model

from sklearn.metrics import f1_score, accuracy_score
def evaluate_bert(model, loader):
    model.eval()
    preds, trues = [], []
    with torch.no_grad():
        for batch in loader:
            input_ids = batch['input_ids'].to(DEVICE)
            attention_mask = batch['attention_mask'].to(DEVICE)
            labels = batch['label'].to(DEVICE)
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            batch_preds = torch.argmax(logits, dim=1).cpu().numpy()
            preds.extend(batch_preds.tolist())
            trues.extend(labels.cpu().numpy().tolist())
    return {
        'acc': accuracy_score(trues, preds),
        'f1': f1_score(trues, preds, zero_division=0)
    }

bert_model = train_bert()
tokenizer.save_pretrained(MODEL_SAVE_DIR)
print("BERT training complete; model + tokenizer saved to", MODEL_SAVE_DIR)

import os
import pandas as pd

train_image_dir = "/content/drive/MyDrive/liar's dataset/real and fake/train"
test_image_dir  = "/content/drive/MyDrive/liar's dataset/real and fake/test"
PROCESSED = "/content/drive/MyDrive/liar's dataset/processed"
os.makedirs(PROCESSED, exist_ok=True)

def make_img_csv(image_dir, csv_path):
    data = []

    for label_name in os.listdir(image_dir):
        label_path = os.path.join(image_dir, label_name)

        if not os.path.isdir(label_path):
            continue

        label = 1 if "REAL" in label_name.upper() else 0

        try:
            files = os.listdir(label_path)
        except OSError as e:
            print(f"âš ï¸ Skipping folder due to IO error: {label_path}")
            continue

        for f in files:
            fpath = os.path.join(label_path, f)
            if os.path.isfile(fpath):
                data.append({
                    "filename": f,
                    "label": label,
                    "path": fpath
                })

    df = pd.DataFrame(data)
    df.to_csv(csv_path, index=False)
    return df

img_train_df = make_img_csv(
    train_image_dir,
    os.path.join(PROCESSED, "img_train.csv")
)

img_test_df = make_img_csv(
    test_image_dir,
    os.path.join(PROCESSED, "img_test.csv")
)

print("âœ… Train images:", len(img_train_df))
print("âœ… Test images:", len(img_test_df))
print(img_train_df.head())

#jaruri cells starts from here
import pandas as pd
import os

train_image_dir = "/content/drive/MyDrive/liar's dataset/real and fake/train"
test_image_dir  = "/content/drive/MyDrive/liar's dataset/real and fake/test"

def make_csv(image_dir, csv_path):
    rows = []
    for label_dir in os.listdir(image_dir):
        label_path = os.path.join(image_dir, label_dir)
        if not os.path.isdir(label_path):
            continue
        label = 1 if 'REAL' in label_dir.upper() else 0
        for f in os.listdir(label_path):
            f_path = os.path.join(label_path, f)
            if os.path.isfile(f_path):
                rows.append({'filename': f, 'label': label, 'path': f_path})
    df = pd.DataFrame(rows)
    df.to_csv(csv_path, index=False)
    return df

img_train_df = make_csv(train_image_dir, os.path.join(PROCESSED, "img_train.csv"))
img_test_df  = make_csv(test_image_dir,  os.path.join(PROCESSED, "img_test.csv"))

print("Train images:", len(img_train_df))
print("Test images:", len(img_test_df))

import timm, torch, os
from torchvision import transforms
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import torch.nn as nn
from sklearn.metrics import accuracy_score
from tqdm.auto import tqdm

IMG_SIZE = 224
BATCH_SIZE_IMG = 8
EPOCHS_IMG = 5
LR_IMG = 1e-4
IMG_MODEL_NAME = 'vit_tiny_patch16_224'
IMG_SAVE_DIR = os.path.join(PROCESSED, "vit_image")
os.makedirs(IMG_SAVE_DIR, exist_ok=True)

transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])

class ImgDataset(Dataset):
    def __init__(self, df, transform):
        self.df = df.reset_index(drop=True)
        self.transform = transform
    def __len__(self): return len(self.df)
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        img = Image.open(row['path']).convert('RGB')
        img = self.transform(img)
        label = int(row['label'])
        return img, torch.tensor(label, dtype=torch.long)

#testing on small size of small as it is easy to run on cpu

train_img_df_small = img_train_df.sample(200, random_state=42).reset_index(drop=True)
test_img_df_small  = img_test_df.sample(100, random_state=42).reset_index(drop=True)


train_img_ds = ImgDataset(train_img_df_small, transform)
test_img_ds  = ImgDataset(test_img_df_small, transform)

train_img_loader = DataLoader(train_img_ds, batch_size=16, shuffle=True, num_workers=2, pin_memory=True)
test_img_loader  = DataLoader(test_img_ds, batch_size=16, shuffle=False, num_workers=2, pin_memory=True)


print("Train loader batches:", len(train_img_loader))
print("Test loader batches:", len(test_img_loader))

class ViTClassifier(nn.Module):
    def __init__(self, model_name=IMG_MODEL_NAME, num_classes=2):
        super().__init__()
        self.vit = timm.create_model(model_name, pretrained=True)
        in_f = self.vit.head.in_features
        self.vit.head = nn.Linear(in_f, num_classes)
    def forward(self, x):
        return self.vit(x)

img_model = ViTClassifier().to(DEVICE)
optimizer_img = torch.optim.AdamW(img_model.parameters(), lr=LR_IMG)
criterion = nn.CrossEntropyLoss()
scaler = torch.cuda.amp.GradScaler() if DEVICE.type=='cuda' else None

def train_img():
    best_acc = 0.0
    for epoch in range(EPOCHS_IMG):
        img_model.train()
        running_loss = 0.0
        for imgs, labels in tqdm(train_img_loader, desc=f"Img Train {epoch+1}"):
            imgs = imgs.to(DEVICE)
            labels = labels.to(DEVICE)
            optimizer_img.zero_grad()
            if scaler:
                with torch.cuda.amp.autocast():
                    logits = img_model(imgs)
                    loss = criterion(logits, labels)
                scaler.scale(loss).backward()
                scaler.step(optimizer_img)
                scaler.update()
            else:
                logits = img_model(imgs)
                loss = criterion(logits, labels)
                loss.backward()
                optimizer_img.step()
            running_loss += loss.item()

        img_model.eval()
        preds, trues = [], []
        with torch.no_grad():
            for imgs, labels in test_img_loader:
                imgs = imgs.to(DEVICE)
                labels = labels.to(DEVICE)
                logits = img_model(imgs)
                p = torch.argmax(logits, dim=1).cpu().numpy()
                preds.extend(p.tolist())
                trues.extend(labels.cpu().numpy().tolist())
        acc = accuracy_score(trues, preds)
        print(f"Epoch {epoch+1} Img val acc: {acc:.4f}")
        if acc > best_acc:
            best_acc = acc
            torch.save(img_model.state_dict(), os.path.join(IMG_SAVE_DIR, "vit_best.pt"))
            print("Saved vit checkpoint")

    img_model.load_state_dict(torch.load(os.path.join(IMG_SAVE_DIR, "vit_best.pt")))
    return img_model

img_model = train_img()
print("Image model trained and saved to", IMG_SAVE_DIR)

!pip install faiss-cpu

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pandas as pd
import os

SRC_CSV = os.path.join(PROCESSED, "merged_dataset.csv")
if not os.path.exists(SRC_CSV):
    print("Warning: merged_dataset.csv not found at", SRC_CSV)
else:
    src_df = pd.read_csv(SRC_CSV)
    print("Merged dataset shape:", src_df.shape)

embed_model = SentenceTransformer('all-MiniLM-L6-v2')

texts = src_df['text'].astype(str).tolist()
print("Generating embeddings for facts...")
embeddings = embed_model.encode(texts, batch_size=64, show_progress_bar=True, convert_to_numpy=True)

faiss.normalize_L2(embeddings)

dim = embeddings.shape[1]
index = faiss.IndexFlatIP(dim)
index.add(embeddings)

faiss.write_index(index, os.path.join(PROCESSED, "faiss_index.idx"))
np.save(os.path.join(PROCESSED, "source_embeddings.npy"), embeddings)
src_df.to_pickle(os.path.join(PROCESSED, "source_meta.pkl"))
print("FAISS index, embeddings, and metadata saved successfully.")

import faiss
import pandas as pd
import os
import numpy as np

faiss_index_path = os.path.join(PROCESSED, "faiss_index.idx")
meta_path = os.path.join(PROCESSED, "source_meta.pkl")

if os.path.exists(faiss_index_path) and os.path.exists(meta_path):
    index = faiss.read_index(faiss_index_path)
    src_df = pd.read_pickle(meta_path)
    print("FAISS index & metadata loaded")
else:
    index = None
    src_df = None
    print("FAISS files missing â€” retrieval disabled")

def retrieve_evidence(query, top_k=5):
    """
    Retrieve top-k most semantically similar facts for a query using FAISS and SentenceTransformer.
    Returns list of dicts: {'text': ..., 'label': ..., 'score': ...}
    """
    q_emb = embed_model.encode([query], convert_to_numpy=True)
    faiss.normalize_L2(q_emb)
    D, I = index.search(q_emb, top_k)

    results = []
    for score, idx in zip(D[0], I[0]):
        label = src_df.iloc[idx]['label']
        if pd.isna(label):
            continue
        results.append({
            'text': src_df.iloc[idx]['text'],
            'label': int(label),
            'score': float(score)
        })
    return results

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from PIL import Image
from torchvision import transforms
import torch.nn.functional as F

bert_ckpt = os.path.join(MODEL_SAVE_DIR, "bert_best.pt")
bert_tokenizer = BertTokenizer.from_pretrained(MODEL_SAVE_DIR)
bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
bert_model.load_state_dict(torch.load(bert_ckpt, map_location=DEVICE))
bert_model.to(DEVICE)
bert_model.eval()

vit_ckpt = os.path.join(IMG_SAVE_DIR, "vit_best.pt")
class ViTClassifier(nn.Module):
    def __init__(self, model_name=IMG_MODEL_NAME, num_classes=2):
        super().__init__()
        self.vit = timm.create_model(model_name, pretrained=True)
        in_f = self.vit.head.in_features
        self.vit.head = nn.Linear(in_f, num_classes)
    def forward(self, x):
        return self.vit(x)

img_model = ViTClassifier()
img_model.load_state_dict(torch.load(vit_ckpt, map_location=DEVICE))
img_model.to(DEVICE)
img_model.eval()

def predict_text_claim(text):
    enc = bert_tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=TEXT_MAX_LEN)
    input_ids = enc['input_ids'].to(DEVICE)
    attention_mask = enc['attention_mask'].to(DEVICE)
    with torch.no_grad():
        logits = bert_model(input_ids=input_ids, attention_mask=attention_mask).logits
        probs = F.softmax(logits, dim=1).cpu().numpy()[0]
        pred = 1 if probs[1] > 0.6 else 0
    evidence = retrieve_evidence(text, top_k=5) if os.path.exists(os.path.join(PROCESSED, "faiss_index.idx")) else []
    return {'label': 'real' if pred==1 else 'fake', 'prob': float(probs[1]), 'evidence': evidence}

inv_transform = transforms.Compose([
    transforms.Resize((IMG_SIZE, IMG_SIZE)),
    transforms.ToTensor(),
    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])
])
def predict_image_file(image_path):
    img = Image.open(image_path).convert('RGB')
    inp = inv_transform(img).unsqueeze(0).to(DEVICE)
    with torch.no_grad():
        logits = img_model(inp)
        probs = F.softmax(logits, dim=1).cpu().numpy()[0]
        pred = 1 if probs[1] > 0.6 else 0
    return {'label': 'real' if pred==1 else 'fake', 'prob': float(probs[1])}

!pip install ftfy regex tqdm
!git clone https://github.com/openai/CLIP.git
!pip install -e CLIP

import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt', quiet=True)

def safe_sent_tokenize(text):
    return sent_tokenize(text, language='english')

THRESHOLDS = {
    'text': 0.60,
    'image': 0.5,
    'evidence': 0.50,
    'combined': 0.45,
    'clip_sim': 0.3
}

def get_weights(text_present=True, image_present=True, evidence_present=True):
    w = {'text':0.5, 'image':0.3, 'evidence':0.2}
    total = 0
    if not text_present:
        w['text'] = 0
    if not image_present:
        w['image'] = 0
    if not evidence_present:
        w['evidence'] = 0
    total = sum(w.values())
    if total > 0:
        w = {k:v/total for k,v in w.items()}  # Normalize
    return w

import sys
import subprocess

def install_clip():
    try:
        import clip
        print("âœ… CLIP already installed")
    except ImportError:
        print("â¬‡ï¸ Installing CLIP from GitHub...")
        subprocess.check_call(
            [sys.executable, "-m", "pip", "install", "git+https://github.com/openai/CLIP.git"],
            stdout=subprocess.DEVNULL
        )
        print("âœ… CLIP installed")

install_clip()

import clip
from PIL import Image
import torch

clip_model, clip_preprocess = clip.load("ViT-B/32", device=DEVICE)

def predict_paragraph_claim(paragraph):
    sentences = safe_sent_tokenize(paragraph)

    probs = []
    all_evidence = []

    for s in sentences:
        res = predict_text_claim(s)
        probs.append(res['prob'])
        all_evidence.extend(res['evidence'])

    seen = set()
    filtered = []
    for ev in all_evidence:
        key = ev['text']
        if key not in seen:
            filtered.append(ev)
            seen.add(key)

    avg_prob = sum(probs) / len(probs) if probs else 0.0
    label = 'real' if avg_prob > THRESHOLDS['text'] else 'fake'

    return {
        'label': label,
        'prob': avg_prob,
        'evidence': filtered
    }
def combined_decision(text=None, image_path=None):
    text_present = text is not None
    image_present = image_path is not None
    evidence_present = False
    results = {}

    if text_present:
        results['text'] = predict_paragraph_claim(text)

    if image_present:
        results['image'] = predict_image_file(image_path)

    evidence_score = 0.0
    if 'text' in results and results['text']['evidence']:
        ev = results['text']['evidence']
        evidence_present = True
        real_count = sum(1 for r in ev if r['label']==1 and r['score']>THRESHOLDS['evidence'])
        evidence_score = real_count / len(ev)

    weights = get_weights(text_present, image_present, evidence_present)

    t = results['text']['prob'] if 'text' in results else 0.0
    i = results['image']['prob'] if 'image' in results else 0.0
    e = evidence_score

    s = 0
    score = 0
    if 'text' in results:
        score += weights['text'] * t
        s += weights['text']
    if 'image' in results:
        score += weights['image'] * i
        s += weights['image']
    if evidence_present:
        score += weights['evidence'] * e
        s += weights['evidence']

    credibility = score / s if s>0 else 0.0

    if text_present and image_present:
        try:
            img = Image.open(image_path).convert('RGB')
            text_tokens = clip.tokenize([text]).to(DEVICE)
            image_input = clip_preprocess(img).unsqueeze(0).to(DEVICE)

            with torch.no_grad():
                img_f = clip_model.encode_image(image_input)
                txt_f = clip_model.encode_text(text_tokens)
                sim = torch.cosine_similarity(img_f, txt_f).item()

            results['clip_similarity'] = sim

            if sim < THRESHOLDS['clip_sim']:
                credibility *= 0.8
        except:
            results['clip_similarity'] = None

    return {'results': results, 'credibility': float(credibility)}

import nltk
nltk.download('punkt_tab')
import nltk
nltk.download('punkt', quiet=True)
from sklearn.metrics import precision_score, recall_score, f1_score
import nltk
nltk.download('punkt')
from nltk.tokenize import sent_tokenize

import nltk
from nltk.tokenize import sent_tokenize

nltk.download('punkt', quiet=True)

def safe_sent_tokenize(text):
    return sent_tokenize(text, language='english')

test_paragraphs = [
    "The Indian Constitution came into effect on January 26, 1950. It established the Republic of India.",
    "Aliens built the pyramids in Egypt."
]

for para in test_paragraphs:
    sentences = safe_sent_tokenize(para)
    print(sentences)

from sklearn.metrics import precision_score, recall_score, f1_score

def evaluate_paragraph(
    texts,
    labels,
    image_paths=None,
    top_k=5
):
    assert len(texts) == len(labels), "Texts and labels must match"

    y_true = labels
    y_pred_text = []
    y_pred_image = []
    y_pred_evidence = []
    y_pred_combined = []

    for i, text in enumerate(texts):

        sentences = safe_sent_tokenize(text)
        probs = [predict_text_claim(s)['prob'] for s in sentences]
        avg_prob = sum(probs)/len(probs)
        y_pred_text.append(1 if avg_prob > THRESHOLDS['text'] else 0)

        img_path = image_paths[i] if image_paths and image_paths[i] else None
        if img_path:
            img_res = predict_image_file(img_path)
            y_pred_image.append(1 if img_res['prob'] > THRESHOLDS['image'] else 0)
        else:
            y_pred_image.append(None)

        ev = retrieve_evidence(text, top_k)
        real_cnt = sum(1 for r in ev if r['label']==1 and r['score']>THRESHOLDS['evidence'])
        ev_score = sum(1 for r in ev if r['label']==1) / len(ev) if ev else 0
        y_pred_evidence.append(1 if ev_score > THRESHOLDS['evidence'] else 0)


        comb = combined_decision(text=text, image_path=img_path)
        y_pred_combined.append(1 if comb['credibility'] > THRESHOLDS['combined'] else 0)

    def metrics(name, preds):
        preds = [p if p is not None else 0 for p in preds]
        print(
            f"{name}: "
            f"P={precision_score(y_true, preds, zero_division=0):.3f} | "
            f"R={recall_score(y_true, preds, zero_division=0):.3f} | "
            f"F1={f1_score(y_true, preds, zero_division=0):.3f}"
        )

    metrics("TEXT", y_pred_text)
    if any(p is not None for p in y_pred_image):
        metrics("IMAGE", y_pred_image)
    metrics("EVIDENCE", y_pred_evidence)
    metrics("COMBINED", y_pred_combined)

import numpy as np
from sklearn.metrics import f1_score

def tune_thresholds(y_true, probs, name="text"):
    best_thr, best_f1 = 0, 0
    for thr in np.arange(0.1, 0.9, 0.05):
        preds = [1 if p>thr else 0 for p in probs]
        f1 = f1_score(y_true, preds, zero_division=0)
        if f1 > best_f1:
            best_f1 = f1
            best_thr = thr
    print(f"Best {name} threshold: {best_thr:.2f}, F1: {best_f1:.3f}")
    return best_thr


test_paragraphs = [
    "The Indian Constitution came into effect on January 26, 1950.",
    "Aliens built the pyramids in Egypt."
]

test_labels = [1, 0]
test_images = [None, None]

evaluate_paragraph(
    texts=test_paragraphs,
    labels=test_labels,
    image_paths=test_images
)

import numpy as np
from sklearn.metrics import f1_score

def tune_thresholds(y_true, probs, name="text"):
    best_thr, best_f1 = 0, 0
    for thr in np.arange(0.1, 0.9, 0.05):
        preds = [1 if p>thr else 0 for p in probs]
        f1 = f1_score(y_true, preds, zero_division=0)
        if f1 > best_f1:
            best_f1 = f1
            best_thr = thr
    print(f"Best {name} threshold: {best_thr:.2f}, F1: {best_f1:.3f}")
    return best_thr

text_probs = [combined_decision(text=t)['results']['text']['prob'] for t in test_paragraphs]
THRESHOLDS['text'] = tune_thresholds(test_labels, text_probs, "text")

image_probs = []
for t, img_path in zip(test_paragraphs, test_images):
    if img_path:
        image_probs.append(predict_image_file(img_path)['prob'])
    else:
        image_probs.append(0)
THRESHOLDS['image'] = tune_thresholds(test_labels, image_probs, "image")

evidence_probs = []
for t in test_paragraphs:
    ev = retrieve_evidence(t)
    score = sum(1 for r in ev if r['label']==1) / len(ev) if ev else 0
    evidence_probs.append(score)
THRESHOLDS['evidence'] = tune_thresholds(test_labels, evidence_probs, "evidence")

combined_probs = [combined_decision(text=t, image_path=i)['credibility'] for t,i in zip(test_paragraphs, test_images)]
THRESHOLDS['combined'] = tune_thresholds(test_labels, combined_probs, "combined")

example_text = "The Indian Constitution came into effect on January 26, 1950."
result = combined_decision(text=example_text, image_path=None)

print("\n--- Text Claim Verification ---")
t_res = result['results']['text']
print(f"Text Prediction: {t_res['label'].upper()}  |  Probability: {t_res['prob']:.4f}")

if t_res['evidence']:
    print("\nTop Evidence from facts database:")
    for ev in t_res['evidence']:
        print(f"- [{ev['label']}] {ev['text'][:120]}... (score: {ev['score']:.4f})")

print(f"\nCredibility Score: {result['credibility']:.4f}")

eval_samples = [
    {
        "text": "The Indian Constitution came into effect on January 26, 1950.",
        "image": None,
        "label": 1
    },
    {
        "text": "Aliens built the pyramids in Egypt.",
        "image": None,
        "label": 0
    },
    {
        "text": "Water boils at 100 degrees Celsius at sea level.",
        "image": None,
        "label": 1
    },
    {
        "text": "Drinking bleach cures COVID-19.",
        "image": None,
        "label": 0
    }
]

from sklearn.metrics import precision_score, recall_score, f1_score

def run_ablation(samples, mode):
    """
    mode options:
    - 'text'
    - 'image'
    - 'evidence'
    - 'text+image'
    - 'text+evidence'
    - 'full'
    """
    y_true = []
    y_pred = []

    for s in samples:
        text = s['text']
        image = s['image']
        label = s['label']

        y_true.append(label)

        t_prob = 0.0
        if 'text' in mode:
            t_res = predict_paragraph_claim(text)
            t_prob = t_res['prob']

        i_prob = 0.0
        if 'image' in mode and image:
            i_res = predict_image_file(image)
            i_prob = i_res['prob']

        e_prob = 0.0
        if 'evidence' in mode:
            ev = retrieve_evidence(text, top_k=5)
            real_hits = sum(
                1 for r in ev if r['label'] == 1 and r['score'] > THRESHOLDS['evidence']
            )
            e_prob = real_hits / len(ev) if ev else 0.0

        probs = []
        if 'text' in mode:
            probs.append(t_prob)
        if 'image' in mode and image:
            probs.append(i_prob)
        if 'evidence' in mode:
            probs.append(e_prob)

        final_prob = sum(probs) / len(probs) if probs else 0.0
        pred = 1 if final_prob > THRESHOLDS['combined'] else 0
        y_pred.append(pred)

    return {
        "precision": precision_score(y_true, y_pred, zero_division=0),
        "recall": recall_score(y_true, y_pred, zero_division=0),
        "f1": f1_score(y_true, y_pred, zero_division=0)
    }

from sklearn.metrics import precision_score, recall_score, f1_score

def run_ablation_experiments(texts, labels, image_paths=None, top_k=5):
    """
    Run ablation experiments on different modality combinations.

    texts       : list of paragraphs
    labels      : list of true labels (0/1)
    image_paths : list of image paths or None
    top_k      : number of top evidence results to consider
    """
    ablations = {
        "Text Only": {"text": True, "image": False, "evidence": False},
        "Image Only": {"text": False, "image": True, "evidence": False},
        "Evidence Only": {"text": False, "image": False, "evidence": True},
        "Text + Image": {"text": True, "image": True, "evidence": False},
        "Text + Evidence": {"text": True, "image": False, "evidence": True},
        "Full System": {"text": True, "image": True, "evidence": True}
    }

    results = {}

    for name, config in ablations.items():
        preds = []
        for i, text in enumerate(texts):
            txt = text if config["text"] else None
            img_path = image_paths[i] if (image_paths and config["image"]) else None

            text_prob = None
            if txt:
                sentences = safe_sent_tokenize(txt)
                text_probs = [predict_text_claim(s)['prob'] for s in sentences]
                text_prob = sum(text_probs)/len(text_probs)

            image_prob = None
            if img_path:
                img_res = predict_image_file(img_path)
                image_prob = img_res['prob']

            evidence_prob = None
            if config["evidence"] and txt:
                ev = retrieve_evidence(txt, top_k)
                if ev:
                    real_cnt = sum(1 for r in ev if r['label']==1 and r['score']>THRESHOLDS['evidence'])
                    evidence_prob = real_cnt / len(ev)
                else:
                    evidence_prob = 0

            if txt or img_path:
                combined_res = combined_decision(text=txt, image_path=img_path)
                combined_pred = 1 if combined_res['credibility'] > THRESHOLDS['combined'] else 0
            else:
                combined_pred = 0

            pred = 0
            if config["text"] and text_prob is not None and text_prob > THRESHOLDS['text']:
                pred = 1
            if config["image"] and image_prob is not None and image_prob > THRESHOLDS['image']:
                pred = 1
            if config["evidence"] and evidence_prob is not None and evidence_prob > 0.5:  # evidence threshold fixed at 0.5
                pred = 1
            if config["text"] and config["image"] and config["evidence"]:
                pred = combined_pred

            preds.append(pred)

        precision = precision_score(labels, preds, zero_division=0)
        recall = recall_score(labels, preds, zero_division=0)
        f1 = f1_score(labels, preds, zero_division=0)
        results[name] = {"precision": precision, "recall": recall, "f1": f1}

        print(f"{name:18s} | P: {precision:.3f} R: {recall:.3f} F1: {f1:.3f}")

    return results

results = run_ablation_experiments(
    texts=test_paragraphs,
    labels=test_labels,
    image_paths=test_images
)

import gradio as gr
import tempfile
import os

def format_output(result):
    credibility = result["credibility"]
    results = result["results"]

    verdict = "REAL âœ…" if credibility > THRESHOLDS["combined"] else "FAKE âŒ"

    details = f"""
### ğŸ§¾ Final Verdict
**{verdict}**

### ğŸ“Š Credibility Score
**{credibility:.3f}**
"""

    if "text" in results:
        details += f"""
### ğŸ“ Text Analysis
- Probability (REAL): **{results['text']['prob']:.3f}**
- Prediction: **{results['text']['label'].upper()}**
"""

    if "image" in results:
        details += f"""
### ğŸ–¼ï¸ Image Analysis
- Probability (REAL): **{results['image']['prob']:.3f}**
- Prediction: **{results['image']['label'].upper()}**
"""

    if "clip_similarity" in results and results["clip_similarity"] is not None:
        details += f"""
### ğŸ”— Textâ€“Image Consistency (CLIP)
- Similarity Score: **{results['clip_similarity']:.3f}**
"""

    return details


def format_evidence(result):
    if "text" not in result["results"]:
        return "No evidence retrieved."

    evidence = result["results"]["text"]["evidence"]
    if not evidence:
        return "No supporting evidence found."

    out = "### ğŸ” Retrieved Evidence\n"
    for i, ev in enumerate(evidence[:5], 1):
        label = "REAL" if ev["label"] == 1 else "FAKE"
        out += f"""
**{i}. [{label}] (score: {ev['score']:.3f})**
{ev['text'][:300]}...
"""
    return out


def run_verification(text, image):
    image_path = None

    if image is not None:
        tmp = tempfile.NamedTemporaryFile(delete=False, suffix=".png")
        image.save(tmp.name)
        image_path = tmp.name

    if not text and not image_path:
        return "âš ï¸ Please provide text and/or image.", ""

    result = combined_decision(
        text=text if text else None,
        image_path=image_path
    )

    summary = format_output(result)
    evidence = format_evidence(result)

    return summary, evidence


with gr.Blocks(theme=gr.themes.Soft()) as demo:

    gr.Markdown("""
# ğŸ§  Multimodal Fake News Verification System
**Text Â· Image Â· Evidence Â· CLIP Consistency**

Enter a claim and optionally upload an image to verify its authenticity.
""")

    with gr.Row():
        with gr.Column(scale=2):
            text_input = gr.Textbox(
                label="ğŸ“ News Claim / Paragraph",
                placeholder="Enter a news claim or paragraph here...",
                lines=6
            )
            image_input = gr.Image(
                label="ğŸ–¼ï¸ Upload Image (optional)",
                type="pil"
            )
            submit_btn = gr.Button("ğŸ” Verify Claim", variant="primary")

        with gr.Column(scale=3):
            output_summary = gr.Markdown(label="Result")
            output_evidence = gr.Markdown(label="Evidence")

    submit_btn.click(
        fn=run_verification,
        inputs=[text_input, image_input],
        outputs=[output_summary, output_evidence]
    )

    gr.Markdown("""
---
### â„¹ï¸ System Notes
- Uses **BERT** for text verification
- **ViT** for image classification
- **FAISS + SentenceTransformers** for evidence retrieval
- **CLIP** for textâ€“image consistency
- Final decision based on **dynamic weighted fusion**
""")

demo.launch(debug=True)



